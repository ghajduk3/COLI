{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bert-notebook.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"15bOkcqoRBYbuSdpA9fVGqUmvGtui8iC0","authorship_tag":"ABX9TyPMeFvS81aDttloI6VSiiun"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"kDMqDAAAmiLD"},"source":["**I/O device register**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":527},"id":"U2q6Ix5Fl2s3","executionInfo":{"status":"error","timestamp":1618676962116,"user_tz":-120,"elapsed":2966,"user":{"displayName":"Gojko Hajduković","photoUrl":"","userId":"07190728708613873917"}},"outputId":"4c495d27-3f8c-4c0b-ff05-753fdf2efa04"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":10,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \"\"\"\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    258\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dfs-auth-dance'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfifo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfifo_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m           \u001b[0mfifo_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_prompt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m       \u001b[0mwrote_to_fifo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"Ek7kHsU3nTBN"},"source":["**Install required libraries**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1yKQ4IG6nXJT","executionInfo":{"status":"ok","timestamp":1618676890305,"user_tz":-120,"elapsed":7506,"user":{"displayName":"Gojko Hajduković","photoUrl":"","userId":"07190728708613873917"}},"outputId":"ea7aa3c2-839c-40e4-ccd0-6104ba02e922"},"source":["!pip3 install transformers"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n","\u001b[K     |████████████████████████████████| 2.1MB 7.5MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n","\u001b[K     |████████████████████████████████| 870kB 42.2MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 52.1MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=0b528834fb0d1a881ea82a2d524c4d6759aa01299842bae72b7d4e4e4c8afa17\n","  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, tokenizers, transformers\n","Successfully installed sacremoses-0.0.44 tokenizers-0.10.2 transformers-4.5.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xN64HZQ1nqfG","executionInfo":{"status":"ok","timestamp":1618676750658,"user_tz":-120,"elapsed":417835,"user":{"displayName":"Gojko Hajduković","photoUrl":"","userId":"07190728708613873917"}},"outputId":"3439d058-c0dc-423e-dff1-d3767a2d78af"},"source":["!pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html \n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","Collecting torch==1.8.0+cu111\n","\u001b[?25l  Downloading https://download.pytorch.org/whl/cu111/torch-1.8.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (1982.2MB)\n","\u001b[K     |█████████████▌                  | 834.1MB 1.4MB/s eta 0:13:53tcmalloc: large alloc 1147494400 bytes == 0x55f58359a000 @  0x7f72e9344615 0x55f54962e06c 0x55f54970deba 0x55f549630e8d 0x55f54972299d 0x55f5496a4fe9 0x55f54969fb0e 0x55f54963277a 0x55f5496a4e50 0x55f54969fb0e 0x55f54963277a 0x55f5496a186a 0x55f5497237c6 0x55f5496a0ee2 0x55f5497237c6 0x55f5496a0ee2 0x55f5497237c6 0x55f5496a0ee2 0x55f5497237c6 0x55f5497a5431 0x55f549706049 0x55f549670c84 0x55f5496318e9 0x55f5496a5ade 0x55f54963269a 0x55f5496a0a45 0x55f54969fe0d 0x55f54963277a 0x55f5496a0a45 0x55f54963269a 0x55f5496a0a45\n","\u001b[K     |█████████████████               | 1055.7MB 1.3MB/s eta 0:11:36tcmalloc: large alloc 1434370048 bytes == 0x55f5c7bf0000 @  0x7f72e9344615 0x55f54962e06c 0x55f54970deba 0x55f549630e8d 0x55f54972299d 0x55f5496a4fe9 0x55f54969fb0e 0x55f54963277a 0x55f5496a4e50 0x55f54969fb0e 0x55f54963277a 0x55f5496a186a 0x55f5497237c6 0x55f5496a0ee2 0x55f5497237c6 0x55f5496a0ee2 0x55f5497237c6 0x55f5496a0ee2 0x55f5497237c6 0x55f5497a5431 0x55f549706049 0x55f549670c84 0x55f5496318e9 0x55f5496a5ade 0x55f54963269a 0x55f5496a0a45 0x55f54969fe0d 0x55f54963277a 0x55f5496a0a45 0x55f54963269a 0x55f5496a0a45\n","\u001b[K     |█████████████████████▋          | 1336.2MB 1.4MB/s eta 0:07:59tcmalloc: large alloc 1792966656 bytes == 0x55f54ca22000 @  0x7f72e9344615 0x55f54962e06c 0x55f54970deba 0x55f549630e8d 0x55f54972299d 0x55f5496a4fe9 0x55f54969fb0e 0x55f54963277a 0x55f5496a4e50 0x55f54969fb0e 0x55f54963277a 0x55f5496a186a 0x55f5497237c6 0x55f5496a0ee2 0x55f5497237c6 0x55f5496a0ee2 0x55f5497237c6 0x55f5496a0ee2 0x55f5497237c6 0x55f5497a5431 0x55f549706049 0x55f549670c84 0x55f5496318e9 0x55f5496a5ade 0x55f54963269a 0x55f5496a0a45 0x55f54969fe0d 0x55f54963277a 0x55f5496a0a45 0x55f54963269a 0x55f5496a0a45\n","\u001b[K     |███████████████████████████▎    | 1691.1MB 1.2MB/s eta 0:03:59tcmalloc: large alloc 2241208320 bytes == 0x55f5b780a000 @  0x7f72e9344615 0x55f54962e06c 0x55f54970deba 0x55f549630e8d 0x55f54972299d 0x55f5496a4fe9 0x55f54969fb0e 0x55f54963277a 0x55f5496a4e50 0x55f54969fb0e 0x55f54963277a 0x55f5496a186a 0x55f5497237c6 0x55f5496a0ee2 0x55f5497237c6 0x55f5496a0ee2 0x55f5497237c6 0x55f5496a0ee2 0x55f5497237c6 0x55f5497a5431 0x55f549706049 0x55f549670c84 0x55f5496318e9 0x55f5496a5ade 0x55f54963269a 0x55f5496a0a45 0x55f54969fe0d 0x55f54963277a 0x55f5496a0a45 0x55f54963269a 0x55f5496a0a45\n","\u001b[K     |████████████████████████████████| 1982.2MB 1.3MB/s eta 0:00:01tcmalloc: large alloc 1982251008 bytes == 0x55f63d16c000 @  0x7f72e93431e7 0x55f549664017 0x55f54962e06c 0x55f54970deba 0x55f549630e8d 0x55f54972299d 0x55f5496a4fe9 0x55f54969fb0e 0x55f54963277a 0x55f5496a0c9e 0x55f54969fb0e 0x55f54963277a 0x55f5496a0c9e 0x55f54969fb0e 0x55f54963277a 0x55f5496a0c9e 0x55f54969fb0e 0x55f54963277a 0x55f5496a0c9e 0x55f54969fb0e 0x55f54963277a 0x55f5496a0c9e 0x55f54963269a 0x55f5496a0c9e 0x55f54969fb0e 0x55f54963277a 0x55f5496a186a 0x55f54969fb0e 0x55f54963277a 0x55f5496a186a 0x55f54969fb0e\n","tcmalloc: large alloc 2477817856 bytes == 0x55f6b33d8000 @  0x7f72e9344615 0x55f54962e06c 0x55f54970deba 0x55f549630e8d 0x55f54972299d 0x55f5496a4fe9 0x55f54969fb0e 0x55f54963277a 0x55f5496a0c9e 0x55f54969fb0e 0x55f54963277a 0x55f5496a0c9e 0x55f54969fb0e 0x55f54963277a 0x55f5496a0c9e 0x55f54969fb0e 0x55f54963277a 0x55f5496a0c9e 0x55f54969fb0e 0x55f54963277a 0x55f5496a0c9e 0x55f54963269a 0x55f5496a0c9e 0x55f54969fb0e 0x55f54963277a 0x55f5496a186a 0x55f54969fb0e 0x55f54963277a 0x55f5496a186a 0x55f54969fb0e 0x55f549632e11\n","\u001b[K     |████████████████████████████████| 1982.2MB 3.9kB/s \n","\u001b[?25hCollecting torchvision==0.9.0+cu111\n","\u001b[?25l  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.9.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (17.6MB)\n","\u001b[K     |████████████████████████████████| 17.6MB 94kB/s \n","\u001b[?25hCollecting torchaudio==0.8.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/9a/4e2e6dbde627ffb8a6d1d4ebc4683edecad1c08099969f1d7760d92175ff/torchaudio-0.8.0-cp37-cp37m-manylinux1_x86_64.whl (1.9MB)\n","\u001b[K     |████████████████████████████████| 1.9MB 9.1MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu111) (3.7.4.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu111) (1.19.5)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.9.0+cu111) (7.1.2)\n","\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.8.0+cu111 which is incompatible.\u001b[0m\n","Installing collected packages: torch, torchvision, torchaudio\n","  Found existing installation: torch 1.8.1+cu101\n","    Uninstalling torch-1.8.1+cu101:\n","      Successfully uninstalled torch-1.8.1+cu101\n","  Found existing installation: torchvision 0.9.1+cu101\n","    Uninstalling torchvision-0.9.1+cu101:\n","      Successfully uninstalled torchvision-0.9.1+cu101\n","Successfully installed torch-1.8.0+cu111 torchaudio-0.8.0 torchvision-0.9.0+cu111\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9Gi3WN8PCvwO"},"source":["## BERT "]},{"cell_type":"code","metadata":{"id":"5ncO0nQDosXU","executionInfo":{"status":"ok","timestamp":1618680371002,"user_tz":-120,"elapsed":2922,"user":{"displayName":"Gojko Hajduković","photoUrl":"","userId":"07190728708613873917"}}},"source":["from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n","import pandas as pd\n","import torch\n","from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n","import numpy as np\n","import time\n","import datetime"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"-gZZzKq7owzA","executionInfo":{"status":"ok","timestamp":1618680372539,"user_tz":-120,"elapsed":576,"user":{"displayName":"Gojko Hajduković","photoUrl":"","userId":"07190728708613873917"}}},"source":["def setup_classifier(\n","    model_name: str,\n","    num_labels: int) -> BertForSequenceClassification:\n","\n","    model = BertForSequenceClassification.from_pretrained(\n","        model_name,\n","        num_labels = num_labels,\n","        output_attentions = False,\n","        output_hidden_states = False,\n","    )\n","    \n","    return model"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"9g4etr1so1f3","executionInfo":{"status":"ok","timestamp":1618680374840,"user_tz":-120,"elapsed":701,"user":{"displayName":"Gojko Hajduković","photoUrl":"","userId":"07190728708613873917"}}},"source":["def setup_data(\n","    model_name: str,\n","    x: pd.DataFrame, \n","    y: pd.DataFrame,\n","    do_lower_case: bool,\n","    max_length: int) -> TensorDataset:\n","\n","    tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case = do_lower_case)\n","\n","    input_ids = []\n","    attention_masks = []\n","\n","    for text in x:\n","        encoded_dict = tokenizer.encode_plus(\n","            text,\n","            add_special_tokens = True,\n","            max_length = max_length,\n","            padding='max_length',\n","            return_attention_mask = True,\n","            return_tensors = 'pt',\n","            truncation = True\n","        )\n","\n","        input_ids.append(encoded_dict['input_ids'])\n","        attention_masks.append(encoded_dict['attention_mask'])\n","    \n","    input_ids = torch.cat(input_ids, dim=0)\n","    attention_masks = torch.cat(attention_masks, dim=0)\n","    labels = torch.tensor(y)\n","\n","    dataset = TensorDataset(input_ids, attention_masks, labels)\n","\n","    return dataset"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hw9DF4H3o5NS","executionInfo":{"status":"ok","timestamp":1618680377967,"user_tz":-120,"elapsed":943,"user":{"displayName":"Gojko Hajduković","photoUrl":"","userId":"07190728708613873917"}}},"source":["def train_classifier(\n","    model: BertForSequenceClassification, \n","    dataset: TensorDataset, \n","    validation_ratio: float,\n","    batch_size: int,\n","    freeze_embeddings_layer: bool,\n","    freeze_encoder_layers: int,\n","    epochs: int) -> (BertForSequenceClassification, list):\n","\n","    device = select_device()\n","\n","    train_size = int(validation_ratio * len(dataset))\n","    val_size = len(dataset) - train_size\n","\n","    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","    train_dataloader = DataLoader(\n","        train_dataset,\n","        sampler = RandomSampler(train_dataset),\n","        batch_size = batch_size\n","    )\n","\n","    validation_dataloader = DataLoader(\n","        val_dataset,\n","        sampler = SequentialSampler(val_dataset),\n","        batch_size = batch_size\n","    )\n","\n","    modules = []\n","\n","    if freeze_embeddings_layer:\n","        modules.append(model.bert.embeddings)\n","    \n","    for i in range(freeze_encoder_layers):\n","        modules.append(model.bert.encoder.layer[i])\n","\n","    for module in modules:\n","        for param in module.parameters():\n","            param.requires_grad = False\n","    \n","    model.to(device)\n","\n","    optimizer = AdamW(\n","        filter(lambda p: p.requires_grad, model.parameters()),\n","        lr = 2e-5,\n","        eps = 1e-8\n","    )\n","\n","    total_steps = len(train_dataloader) * epochs\n","\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer, \n","        num_warmup_steps = 0,\n","        num_training_steps = total_steps\n","    )\n","\n","    training_stats = []\n","\n","    total_t0 = time.time()\n","\n","    for epoch_i in range(0, epochs):\n","\n","        print(\"\")\n","        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","        print('Training...')\n","\n","        t0 = time.time()\n","\n","        total_train_loss = 0\n","\n","        model.train()\n","\n","        for step, batch in enumerate(train_dataloader):\n","\n","            if step % 40 == 0 and not step == 0:\n","                elapsed = format_time(time.time() - t0)\n","                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","            b_input_ids = batch[0].to(device)\n","            b_input_mask = batch[1].to(device)\n","            b_labels = batch[2].to(device)\n","\n","            model.zero_grad()        \n","\n","            outputs = model(\n","                b_input_ids, \n","                token_type_ids = None, \n","                attention_mask = b_input_mask, \n","                labels = b_labels\n","            )\n","\n","            loss = outputs.loss\n","            logits = outputs.logits\n","\n","            total_train_loss += loss.item()\n","\n","            loss.backward()\n","\n","            # Clip the norm of the gradients to 1.0.\n","            # This is to help prevent the \"exploding gradients\" problem.\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","            optimizer.step()\n","\n","            scheduler.step()\n","\n","        avg_train_loss = total_train_loss / len(train_dataloader)            \n","        \n","        training_time = format_time(time.time() - t0)\n","\n","        print(\"\")\n","        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","        print(\"  Training epcoh took: {:}\".format(training_time))\n","        \n","\n","        print(\"\")\n","        print(\"Running Validation...\")\n","\n","        t0 = time.time()\n","\n","        model.eval()\n","\n","        total_eval_accuracy = 0\n","        total_eval_loss = 0\n","        nb_eval_steps = 0\n","\n","        for batch in validation_dataloader:\n","            \n","            b_input_ids = batch[0].to(device)\n","            b_input_mask = batch[1].to(device)\n","            b_labels = batch[2].to(device)\n","            \n","            with torch.no_grad():\n","\n","                outputs = model(\n","                    b_input_ids, \n","                    token_type_ids = None, \n","                    attention_mask = b_input_mask,\n","                    labels = b_labels\n","                )\n","                \n","                loss = outputs.loss\n","                logits = outputs.logits\n","                \n","            total_eval_loss += loss.item()\n","\n","            logits = logits.detach().cpu().numpy()\n","            label_ids = b_labels.cpu().numpy()\n","\n","            total_eval_accuracy += flat_accuracy(logits, label_ids)\n","            \n","\n","        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n","        print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","\n","        avg_val_loss = total_eval_loss / len(validation_dataloader)\n","        \n","        validation_time = format_time(time.time() - t0)\n","        \n","        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","        print(\"  Validation took: {:}\".format(validation_time))\n","\n","        training_stats.append(\n","            {\n","                'epoch': epoch_i + 1,\n","                'Training Loss': avg_train_loss,\n","                'Valid. Loss': avg_val_loss,\n","                'Valid. Accur.': avg_val_accuracy,\n","                'Training Time': training_time,\n","                'Validation Time': validation_time\n","            }\n","        )\n","\n","    print(\"\")\n","    print(\"Training complete!\")\n","\n","    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n","\n","    return model, training_stats"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"P_1G3NNIo-P0","executionInfo":{"status":"ok","timestamp":1618680384115,"user_tz":-120,"elapsed":747,"user":{"displayName":"Gojko Hajduković","photoUrl":"","userId":"07190728708613873917"}}},"source":["def test_classifier(\n","    model: BertForSequenceClassification, \n","    dataset: TensorDataset,\n","    batch_size: int):\n","\n","    device = select_device()\n","\n","    prediction_dataloader = DataLoader(\n","        dataset, \n","        sampler = SequentialSampler(dataset), \n","        batch_size = batch_size\n","    )\n","\n","    print(\"\")\n","    print(\"Running Prediction...\")\n","\n","    model.eval()\n","\n","    predictions , true_labels = [], []\n","\n","    for batch in prediction_dataloader:\n","\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2]\n","        \n","        with torch.no_grad():\n","\n","            outputs = model(\n","                b_input_ids, \n","                token_type_ids = None, \n","                attention_mask = b_input_mask\n","            )\n","\n","        logits = outputs.logits\n","\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.numpy()\n","        \n","        predictions.append(logits)\n","        true_labels.append(label_ids)\n","    \n","    print('DONE.')\n","\n","    return predictions, true_labels\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"mrH3biqlpGgY","executionInfo":{"status":"ok","timestamp":1618680386669,"user_tz":-120,"elapsed":676,"user":{"displayName":"Gojko Hajduković","photoUrl":"","userId":"07190728708613873917"}}},"source":["def save_checkpoint(path, model, optimizer, epoch, loss):\n","    torch.save({\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'loss': loss\n","        }, path)\n","\n","\n","def save_model(path, model):\n","    torch.save(model.state_dict(), path)\n","\n","\n","def load_checkpoint(path):\n","    checkpoint = torch.load(path)\n","    return checkpoint['model_state_dict'], checkpoint['optimizer_state_dict'], checkpoint['epoch'], checkpoint['loss']\n","\n","\n","def load_model(path):\n","    return torch.load(path)\n","\n","\n","def select_device():\n","\n","    if torch.cuda.is_available():\n","        print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","        print('We will use the GPU:', torch.cuda.get_device_name(0))\n","        device = torch.device(\"cuda\")\n","    else:\n","        print('No GPU available, using the CPU instead.')\n","        device = torch.device(\"cpu\")\n","    \n","    return device\n","\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    elapsed_rounded = int(round((elapsed)))\n","    return str(datetime.timedelta(seconds=elapsed_rounded))\n","\n","\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GLaSebQ-JJ4C"},"source":["**Accuracy score**\n","\n"]},{"cell_type":"code","metadata":{"id":"gwkR6pndJUjz","executionInfo":{"status":"ok","timestamp":1618685024173,"user_tz":-120,"elapsed":563,"user":{"displayName":"Gojko Hajduković","photoUrl":"","userId":"07190728708613873917"}}},"source":["def avg_accuracy(predictions,labels):\n","  return sum([flat_accuracy(predictions[ind],labels[ind]) for ind in range(len(predictions)) ]) / len(predictions) "],"execution_count":38,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2cUR_wcTC5-p"},"source":["## Training and evaluation"]},{"cell_type":"markdown","metadata":{"id":"1jUkaGyWxnjC"},"source":["**Load binary dataset**"]},{"cell_type":"code","metadata":{"id":"ugdgb93AqdAV","executionInfo":{"status":"ok","timestamp":1618680629864,"user_tz":-120,"elapsed":718,"user":{"displayName":"Gojko Hajduković","photoUrl":"","userId":"07190728708613873917"}}},"source":["binary_data = pd.read_csv('drive/MyDrive/Data/Binary/data.csv')\n","binary_data = binary_data.dropna()\n","binary_data.reset_index(drop=True, inplace=True)\n","x = binary_data['preprocessed']\n","y = binary_data['Label']\n","\n"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HzVHRZEXySqO","executionInfo":{"status":"ok","timestamp":1618684184879,"user_tz":-120,"elapsed":1139522,"user":{"displayName":"Gojko Hajduković","photoUrl":"","userId":"07190728708613873917"}},"outputId":"540d6b40-4c6e-461c-a24f-d9b4cee8db09"},"source":["    model = setup_classifier(\n","        model_name = \"drive/MyDrive/classifiers/bert/CroSloEngual\",\n","        num_labels = 2\n","    )\n","    \n","    # model.load_state_dict(bert.load_model(\"models/m1.pt\"))\n","    \n","    dataset = setup_data(\n","        model_name = \"drive/MyDrive/classifiers/bert/CroSloEngual\",\n","        x = x,\n","        y = y,\n","        do_lower_case = False,\n","        max_length = 180\n","    )\n","    \n","    test_ratio = 0.8\n","    train_size = int(test_ratio * len(dataset))\n","    test_size = len(dataset) - train_size\n","\n","    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n","\n","    \n","    model, stats = train_classifier(\n","        model = model,\n","        dataset = train_dataset,\n","        validation_ratio = 0.9,\n","        batch_size = 32,\n","        freeze_embeddings_layer = True,\n","        freeze_encoder_layers = 8,\n","        epochs = 1\n","    )\n","    \n","    predictions, true_labels = test_classifier(\n","        model = model,\n","        dataset = test_dataset,\n","        batch_size = 32\n","    )"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at drive/MyDrive/classifiers/bert/CroSloEngual were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at drive/MyDrive/classifiers/bert/CroSloEngual and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla T4\n","\n","======== Epoch 1 / 1 ========\n","Training...\n","  Batch    40  of  1,514.    Elapsed: 0:00:24.\n","  Batch    80  of  1,514.    Elapsed: 0:00:47.\n","  Batch   120  of  1,514.    Elapsed: 0:01:09.\n","  Batch   160  of  1,514.    Elapsed: 0:01:32.\n","  Batch   200  of  1,514.    Elapsed: 0:01:56.\n","  Batch   240  of  1,514.    Elapsed: 0:02:18.\n","  Batch   280  of  1,514.    Elapsed: 0:02:41.\n","  Batch   320  of  1,514.    Elapsed: 0:03:04.\n","  Batch   360  of  1,514.    Elapsed: 0:03:27.\n","  Batch   400  of  1,514.    Elapsed: 0:03:50.\n","  Batch   440  of  1,514.    Elapsed: 0:04:13.\n","  Batch   480  of  1,514.    Elapsed: 0:04:36.\n","  Batch   520  of  1,514.    Elapsed: 0:04:58.\n","  Batch   560  of  1,514.    Elapsed: 0:05:21.\n","  Batch   600  of  1,514.    Elapsed: 0:05:44.\n","  Batch   640  of  1,514.    Elapsed: 0:06:07.\n","  Batch   680  of  1,514.    Elapsed: 0:06:30.\n","  Batch   720  of  1,514.    Elapsed: 0:06:53.\n","  Batch   760  of  1,514.    Elapsed: 0:07:16.\n","  Batch   800  of  1,514.    Elapsed: 0:07:38.\n","  Batch   840  of  1,514.    Elapsed: 0:08:01.\n","  Batch   880  of  1,514.    Elapsed: 0:08:24.\n","  Batch   920  of  1,514.    Elapsed: 0:08:47.\n","  Batch   960  of  1,514.    Elapsed: 0:09:10.\n","  Batch 1,000  of  1,514.    Elapsed: 0:09:32.\n","  Batch 1,040  of  1,514.    Elapsed: 0:09:55.\n","  Batch 1,080  of  1,514.    Elapsed: 0:10:18.\n","  Batch 1,120  of  1,514.    Elapsed: 0:10:41.\n","  Batch 1,160  of  1,514.    Elapsed: 0:11:03.\n","  Batch 1,200  of  1,514.    Elapsed: 0:11:26.\n","  Batch 1,240  of  1,514.    Elapsed: 0:11:49.\n","  Batch 1,280  of  1,514.    Elapsed: 0:12:12.\n","  Batch 1,320  of  1,514.    Elapsed: 0:12:35.\n","  Batch 1,360  of  1,514.    Elapsed: 0:12:57.\n","  Batch 1,400  of  1,514.    Elapsed: 0:13:20.\n","  Batch 1,440  of  1,514.    Elapsed: 0:13:43.\n","  Batch 1,480  of  1,514.    Elapsed: 0:14:06.\n","\n","  Average training loss: 0.31\n","  Training epcoh took: 0:14:25\n","\n","Running Validation...\n","  Accuracy: 0.91\n","  Validation Loss: 0.26\n","  Validation took: 0:01:03\n","\n","Training complete!\n","Total training took 0:15:28 (h:mm:ss)\n","There are 1 GPU(s) available.\n","We will use the GPU: Tesla T4\n","\n","Running Prediction...\n","DONE.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZiYaudaVKXo_"},"source":["**Average accuracy**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dsmDx5DXIlV4","executionInfo":{"status":"ok","timestamp":1618684952541,"user_tz":-120,"elapsed":581,"user":{"displayName":"Gojko Hajduković","photoUrl":"","userId":"07190728708613873917"}},"outputId":"b9e4210e-aa30-452c-875b-18a8c9d5041a"},"source":["avg_accuracy(predictions,true_labels)"],"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9068226162198847"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"markdown","metadata":{"id":"YSanatpqKcOz"},"source":["**Saving model**"]},{"cell_type":"code","metadata":{"id":"aGNAQXvpzH2j","executionInfo":{"status":"ok","timestamp":1618684192032,"user_tz":-120,"elapsed":2957,"user":{"displayName":"Gojko Hajduković","photoUrl":"","userId":"07190728708613873917"}}},"source":["save_model(\"drive/MyDrive/models/binary.pt\", model)"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u3LcFKZKJ_l3"},"source":["**Multi-class dataset**"]},{"cell_type":"code","metadata":{"id":"Gkb5Ox29KEkS","executionInfo":{"status":"ok","timestamp":1618686313195,"user_tz":-120,"elapsed":806,"user":{"displayName":"Gojko Hajduković","photoUrl":"","userId":"07190728708613873917"}}},"source":["multiclass_data = pd.read_csv('drive/MyDrive/Data/Multiclass/data.csv')\n","multiclass_data = binary_data.dropna()\n","multiclass_data.reset_index(drop=True, inplace=True)\n","x = multiclass_data['preprocessed']\n","y = multiclass_data['Label']"],"execution_count":40,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1qQq7zVFKQup","executionInfo":{"status":"ok","timestamp":1618690349878,"user_tz":-120,"elapsed":4022974,"user":{"displayName":"Gojko Hajduković","photoUrl":"","userId":"07190728708613873917"}},"outputId":"6ae426e1-383b-4b77-8b8a-2cb6cdb8aa33"},"source":["    model = setup_classifier(\n","        model_name = \"drive/MyDrive/classifiers/bert/CroSloEngual\",\n","        num_labels = 2\n","    )\n","    \n","    # model.load_state_dict(bert.load_model(\"models/m1.pt\"))\n","    \n","    dataset = setup_data(\n","        model_name = \"drive/MyDrive/classifiers/bert/CroSloEngual\",\n","        x = x,\n","        y = y,\n","        do_lower_case = False,\n","        max_length = 180\n","    )\n","    \n","    test_ratio = 0.8\n","    train_size = int(test_ratio * len(dataset))\n","    test_size = len(dataset) - train_size\n","\n","    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n","\n","    \n","    model, stats = train_classifier(\n","        model = model,\n","        dataset = train_dataset,\n","        validation_ratio = 0.9,\n","        batch_size = 32,\n","        freeze_embeddings_layer = False,\n","        freeze_encoder_layers = 0,\n","        epochs = 3\n","    )\n","    \n","    predictions, true_labels = test_classifier(\n","        model = model,\n","        dataset = test_dataset,\n","        batch_size = 32\n","    )"],"execution_count":41,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at drive/MyDrive/classifiers/bert/CroSloEngual were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at drive/MyDrive/classifiers/bert/CroSloEngual and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla T4\n","\n","======== Epoch 1 / 3 ========\n","Training...\n","  Batch    40  of  1,514.    Elapsed: 0:00:33.\n","  Batch    80  of  1,514.    Elapsed: 0:01:05.\n","  Batch   120  of  1,514.    Elapsed: 0:01:37.\n","  Batch   160  of  1,514.    Elapsed: 0:02:09.\n","  Batch   200  of  1,514.    Elapsed: 0:02:41.\n","  Batch   240  of  1,514.    Elapsed: 0:03:13.\n","  Batch   280  of  1,514.    Elapsed: 0:03:45.\n","  Batch   320  of  1,514.    Elapsed: 0:04:17.\n","  Batch   360  of  1,514.    Elapsed: 0:04:49.\n","  Batch   400  of  1,514.    Elapsed: 0:05:21.\n","  Batch   440  of  1,514.    Elapsed: 0:05:53.\n","  Batch   480  of  1,514.    Elapsed: 0:06:25.\n","  Batch   520  of  1,514.    Elapsed: 0:06:56.\n","  Batch   560  of  1,514.    Elapsed: 0:07:28.\n","  Batch   600  of  1,514.    Elapsed: 0:08:00.\n","  Batch   640  of  1,514.    Elapsed: 0:08:32.\n","  Batch   680  of  1,514.    Elapsed: 0:09:04.\n","  Batch   720  of  1,514.    Elapsed: 0:09:36.\n","  Batch   760  of  1,514.    Elapsed: 0:10:08.\n","  Batch   800  of  1,514.    Elapsed: 0:10:39.\n","  Batch   840  of  1,514.    Elapsed: 0:11:11.\n","  Batch   880  of  1,514.    Elapsed: 0:11:43.\n","  Batch   920  of  1,514.    Elapsed: 0:12:15.\n","  Batch   960  of  1,514.    Elapsed: 0:12:47.\n","  Batch 1,000  of  1,514.    Elapsed: 0:13:19.\n","  Batch 1,040  of  1,514.    Elapsed: 0:13:51.\n","  Batch 1,080  of  1,514.    Elapsed: 0:14:23.\n","  Batch 1,120  of  1,514.    Elapsed: 0:14:55.\n","  Batch 1,160  of  1,514.    Elapsed: 0:15:26.\n","  Batch 1,200  of  1,514.    Elapsed: 0:15:58.\n","  Batch 1,240  of  1,514.    Elapsed: 0:16:30.\n","  Batch 1,280  of  1,514.    Elapsed: 0:17:02.\n","  Batch 1,320  of  1,514.    Elapsed: 0:17:34.\n","  Batch 1,360  of  1,514.    Elapsed: 0:18:06.\n","  Batch 1,400  of  1,514.    Elapsed: 0:18:38.\n","  Batch 1,440  of  1,514.    Elapsed: 0:19:10.\n","  Batch 1,480  of  1,514.    Elapsed: 0:19:42.\n","\n","  Average training loss: 0.29\n","  Training epcoh took: 0:20:08\n","\n","Running Validation...\n","  Accuracy: 0.91\n","  Validation Loss: 0.25\n","  Validation took: 0:01:03\n","\n","======== Epoch 2 / 3 ========\n","Training...\n","  Batch    40  of  1,514.    Elapsed: 0:00:32.\n","  Batch    80  of  1,514.    Elapsed: 0:01:04.\n","  Batch   120  of  1,514.    Elapsed: 0:01:36.\n","  Batch   160  of  1,514.    Elapsed: 0:02:08.\n","  Batch   200  of  1,514.    Elapsed: 0:02:40.\n","  Batch   240  of  1,514.    Elapsed: 0:03:12.\n","  Batch   280  of  1,514.    Elapsed: 0:03:43.\n","  Batch   320  of  1,514.    Elapsed: 0:04:15.\n","  Batch   360  of  1,514.    Elapsed: 0:04:47.\n","  Batch   400  of  1,514.    Elapsed: 0:05:19.\n","  Batch   440  of  1,514.    Elapsed: 0:05:51.\n","  Batch   480  of  1,514.    Elapsed: 0:06:23.\n","  Batch   520  of  1,514.    Elapsed: 0:06:55.\n","  Batch   560  of  1,514.    Elapsed: 0:07:27.\n","  Batch   600  of  1,514.    Elapsed: 0:07:59.\n","  Batch   640  of  1,514.    Elapsed: 0:08:31.\n","  Batch   680  of  1,514.    Elapsed: 0:09:03.\n","  Batch   720  of  1,514.    Elapsed: 0:09:35.\n","  Batch   760  of  1,514.    Elapsed: 0:10:07.\n","  Batch   800  of  1,514.    Elapsed: 0:10:39.\n","  Batch   840  of  1,514.    Elapsed: 0:11:11.\n","  Batch   880  of  1,514.    Elapsed: 0:11:43.\n","  Batch   920  of  1,514.    Elapsed: 0:12:15.\n","  Batch   960  of  1,514.    Elapsed: 0:12:47.\n","  Batch 1,000  of  1,514.    Elapsed: 0:13:19.\n","  Batch 1,040  of  1,514.    Elapsed: 0:13:51.\n","  Batch 1,080  of  1,514.    Elapsed: 0:14:23.\n","  Batch 1,120  of  1,514.    Elapsed: 0:14:55.\n","  Batch 1,160  of  1,514.    Elapsed: 0:15:26.\n","  Batch 1,200  of  1,514.    Elapsed: 0:15:58.\n","  Batch 1,240  of  1,514.    Elapsed: 0:16:30.\n","  Batch 1,280  of  1,514.    Elapsed: 0:17:02.\n","  Batch 1,320  of  1,514.    Elapsed: 0:17:34.\n","  Batch 1,360  of  1,514.    Elapsed: 0:18:06.\n","  Batch 1,400  of  1,514.    Elapsed: 0:18:38.\n","  Batch 1,440  of  1,514.    Elapsed: 0:19:10.\n","  Batch 1,480  of  1,514.    Elapsed: 0:19:42.\n","\n","  Average training loss: 0.23\n","  Training epcoh took: 0:20:09\n","\n","Running Validation...\n","  Accuracy: 0.91\n","  Validation Loss: 0.25\n","  Validation took: 0:01:03\n","\n","======== Epoch 3 / 3 ========\n","Training...\n","  Batch    40  of  1,514.    Elapsed: 0:00:32.\n","  Batch    80  of  1,514.    Elapsed: 0:01:04.\n","  Batch   120  of  1,514.    Elapsed: 0:01:36.\n","  Batch   160  of  1,514.    Elapsed: 0:02:08.\n","  Batch   200  of  1,514.    Elapsed: 0:02:39.\n","  Batch   240  of  1,514.    Elapsed: 0:03:11.\n","  Batch   280  of  1,514.    Elapsed: 0:03:43.\n","  Batch   320  of  1,514.    Elapsed: 0:04:15.\n","  Batch   360  of  1,514.    Elapsed: 0:04:47.\n","  Batch   400  of  1,514.    Elapsed: 0:05:19.\n","  Batch   440  of  1,514.    Elapsed: 0:05:50.\n","  Batch   480  of  1,514.    Elapsed: 0:06:22.\n","  Batch   520  of  1,514.    Elapsed: 0:06:54.\n","  Batch   560  of  1,514.    Elapsed: 0:07:26.\n","  Batch   600  of  1,514.    Elapsed: 0:07:58.\n","  Batch   640  of  1,514.    Elapsed: 0:08:30.\n","  Batch   680  of  1,514.    Elapsed: 0:09:02.\n","  Batch   720  of  1,514.    Elapsed: 0:09:34.\n","  Batch   760  of  1,514.    Elapsed: 0:10:05.\n","  Batch   800  of  1,514.    Elapsed: 0:10:37.\n","  Batch   840  of  1,514.    Elapsed: 0:11:09.\n","  Batch   880  of  1,514.    Elapsed: 0:11:41.\n","  Batch   920  of  1,514.    Elapsed: 0:12:13.\n","  Batch   960  of  1,514.    Elapsed: 0:12:45.\n","  Batch 1,000  of  1,514.    Elapsed: 0:13:17.\n","  Batch 1,040  of  1,514.    Elapsed: 0:13:49.\n","  Batch 1,080  of  1,514.    Elapsed: 0:14:20.\n","  Batch 1,120  of  1,514.    Elapsed: 0:14:52.\n","  Batch 1,160  of  1,514.    Elapsed: 0:15:24.\n","  Batch 1,200  of  1,514.    Elapsed: 0:15:56.\n","  Batch 1,240  of  1,514.    Elapsed: 0:16:28.\n","  Batch 1,280  of  1,514.    Elapsed: 0:17:00.\n","  Batch 1,320  of  1,514.    Elapsed: 0:17:32.\n","  Batch 1,360  of  1,514.    Elapsed: 0:18:03.\n","  Batch 1,400  of  1,514.    Elapsed: 0:18:35.\n","  Batch 1,440  of  1,514.    Elapsed: 0:19:07.\n","  Batch 1,480  of  1,514.    Elapsed: 0:19:39.\n","\n","  Average training loss: 0.21\n","  Training epcoh took: 0:20:06\n","\n","Running Validation...\n","  Accuracy: 0.91\n","  Validation Loss: 0.25\n","  Validation took: 0:01:03\n","\n","Training complete!\n","Total training took 1:03:32 (h:mm:ss)\n","There are 1 GPU(s) available.\n","We will use the GPU: Tesla T4\n","\n","Running Prediction...\n","DONE.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kTUBu-IYKpKV"},"source":["**Average accuracy**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gnZA-iFjKVcz","executionInfo":{"status":"ok","timestamp":1618690375205,"user_tz":-120,"elapsed":676,"user":{"displayName":"Gojko Hajduković","photoUrl":"","userId":"07190728708613873917"}},"outputId":"bc022ec0-f32c-4269-f27d-b0119a4ef751"},"source":["avg_accuracy(predictions,true_labels)"],"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9107885137427892"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"markdown","metadata":{"id":"6sxFc2YnKxNu"},"source":["**Saving model**"]},{"cell_type":"code","metadata":{"id":"X2-tX5WrK69X","executionInfo":{"status":"ok","timestamp":1618690383133,"user_tz":-120,"elapsed":2654,"user":{"displayName":"Gojko Hajduković","photoUrl":"","userId":"07190728708613873917"}}},"source":["save_model(\"drive/MyDrive/models/multiclass.pt\", model)"],"execution_count":43,"outputs":[]}]}